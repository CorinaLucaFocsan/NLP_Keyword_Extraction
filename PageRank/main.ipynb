{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install datasets transformers\n",
    "#%pip install -r requirements.txt\n",
    "#%pip install stanfordcorenlp\n",
    "#%pip install stemming\n",
    "#%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data imports\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Page Rank imports\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet as wn\n",
    "import networkx as nx\n",
    "\n",
    "from position_rank import position_rank\n",
    "from tokenizer import StanfordCoreNlpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"midas/duc2001\", \"raw\")\n",
    "data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 308 entries, 0 to 307\n",
      "Data columns (total 6 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   id                      308 non-null    object\n",
      " 1   document                308 non-null    object\n",
      " 2   doc_bio_tags            308 non-null    object\n",
      " 3   extractive_keyphrases   308 non-null    object\n",
      " 4   abstractive_keyphrases  308 non-null    object\n",
      " 5   other_metadata          308 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 14.6+ KB\n",
      "None\n",
      "----\n",
      "              id                                           document  \\\n",
      "0  AP881222-0089  [Here, ,, at, a, glance, ,, are, developments,...   \n",
      "1  AP880331-0140  [Rumbling, spring, thunderstorms, have, announ...   \n",
      "2  AP880629-0159  [Two, U.S., Air, Force, F-16, fighter, jets, c...   \n",
      "3  AP881216-0017  [A, recommended, halt, to, the, government, 's...   \n",
      "4  AP880801-0195  [A, split, ,, charred, tree, stump, is, a, clu...   \n",
      "\n",
      "                                        doc_bio_tags  \\\n",
      "0  [O, O, O, O, O, O, O, O, O, O, O, B, O, B, I, ...   \n",
      "1  [O, B, I, O, O, O, O, O, O, O, B, I, O, O, O, ...   \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "3  [O, B, I, O, O, O, O, O, O, O, O, O, B, I, I, ...   \n",
      "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "\n",
      "                               extractive_keyphrases  \\\n",
      "0  [pan american world airways flight 103, crash,...   \n",
      "1  [tornado season, spring thunderstorms, tornado...   \n",
      "2  [crashes, pilots, bodenheim, training mission,...   \n",
      "3  [forest fire policy, western fire season, nati...   \n",
      "4  [forest fires, criminal investigator, investig...   \n",
      "\n",
      "                              abstractive_keyphrases  \\\n",
      "0  [terrorist threats, widespread wreckage, radic...   \n",
      "1                                                 []   \n",
      "2  [in-flight crash, u.s. air force, f-16 fighter...   \n",
      "3                                                 []   \n",
      "4                                                 []   \n",
      "\n",
      "                 other_metadata  \n",
      "0  {'text': [], 'bio_tags': []}  \n",
      "1  {'text': [], 'bio_tags': []}  \n",
      "2  {'text': [], 'bio_tags': []}  \n",
      "3  {'text': [], 'bio_tags': []}  \n",
      "4  {'text': [], 'bio_tags': []}  \n"
     ]
    }
   ],
   "source": [
    "df = data.data.to_pandas()\n",
    "\n",
    "print(df.info())\n",
    "print('----')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag words, find synset of word and create new list\n",
    "\n",
    "synset_documents = [] # list of the list of synsets for each document\n",
    "for doc in df['document']:\n",
    "    doc_sets = [] # for this document, the list of synsets\n",
    "\n",
    "    for word in doc:\n",
    "        #print(word)\n",
    "        pos_tag = nltk.pos_tag([word])[0][1]\n",
    "        #doc_tags.append(pos_tag)\n",
    "        #print(pos_tag)\n",
    "        \n",
    "        if pos_tag.__contains__('NN'):\n",
    "            part = wn.NOUN\n",
    "        elif pos_tag.__contains__('V'):\n",
    "            part = wn.VERB  \n",
    "        elif pos_tag.__contains__('JJ'):\n",
    "            part = wn.ADJ\n",
    "        elif pos_tag.__contains__('RB'):\n",
    "            part = wn.ADV\n",
    "        else:\n",
    "            continue\n",
    "        synset = wn.synsets(word, pos=part)\n",
    "        #print(synset)\n",
    "        \n",
    "        if len(synset) > 0:\n",
    "            doc_sets.append(synset[0]) # append the synset for this word\n",
    "\n",
    "    synset_documents.append(doc_sets)\n",
    "    \n",
    "\n",
    "#df.insert(6, \"POS\", tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 178 nodes and 15931 edges\n"
     ]
    }
   ],
   "source": [
    "# Construct weighted graph of synsets\n",
    "\n",
    "#print(one.path_similarity(two))\n",
    "\n",
    "doc = 1\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for i in range(len(synset_documents[doc])):\n",
    "    for j in range(len(synset_documents[doc])):\n",
    "        one = synset_documents[doc][i]\n",
    "        two = synset_documents[doc][j]\n",
    "        G.add_edge(one, two, weight=one.path_similarity(two))\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rumble', 'spring', 'thunderstorm', 'have', 'announce', 'get_down', 'unofficial', 'tornado', 'season', 'run']\n"
     ]
    }
   ],
   "source": [
    "# Apply PageRank algorithm on the graph\n",
    "rank = nx.pagerank(G)\n",
    "\n",
    "key_words = list(rank.keys())[:10]\n",
    "\n",
    "for i in range(len(key_words)):\n",
    "    key_words[i] = key_words[i].name().split(\".\")[0]\n",
    "\n",
    "print(key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.4\n",
      "0.5\n",
      "0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Compare pagerank results to extractive keyphrases\n",
    "answers = \" \".join(df['extractive_keyphrases'][doc])\n",
    "\n",
    "hit_rate = 0\n",
    "\n",
    "for word in key_words:\n",
    "    if word in answers:\n",
    "        hit_rate += 1\n",
    "\n",
    "precision = hit_rate / len(key_words)\n",
    "recall = hit_rate / len(df['extractive_keyphrases'][doc])\n",
    "f1 = 2/((1/precision) + (1/recall))\n",
    "\n",
    "print(hit_rate)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tornado season' 'spring thunderstorms' 'tornadoes' 'texas'\n",
      " 'property damage' 'tornado warning' 'tornado watches' 'disaster research']\n"
     ]
    }
   ],
   "source": [
    "print(df['extractive_keyphrases'][doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position Rank\n",
    "\n",
    "title = \"PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents\"\n",
    "abstract = \"\"\"The large and growing amounts of online scholarly data present both challenges and\n",
    "opportunities to enhance knowledge discovery. One such challenge is to automatically extract a small set of keyphrases\n",
    "from a document that can accurately describe the document’s content and can facilitate fast information processing. In\n",
    "this paper, we propose PositionRank, an\n",
    "unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a\n",
    "word’s occurrences into a biased PageRank. Our model obtains remarkable improvements in performance over PageRank models that do not take into account\n",
    "word positions as well as over strong baselines for this task. Specifically, on several\n",
    "datasets of research papers, PositionRank\n",
    "achieves improvements as high as 29.09%.\"\"\"\n",
    "document = df['document'][1]\n",
    "\n",
    "tokenizer = StanfordCoreNlpTokenizer(\"http://localhost\", port = 9000)\n",
    "position_rank(title + \" \" + abstract, tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
