{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load datasets\n",
    "duc2001_dataset = load_dataset(\"midas/duc2001\", \"raw\")[\"test\"]\n",
    "inspec_dataset = load_dataset(\"midas/inspec\", \"raw\")[\"test\"]\n",
    "nus_dataset = load_dataset(\"midas/nus\", \"raw\")[\"test\"]\n",
    "\n",
    "def dataset_to_dataframe(dataset):\n",
    "    return pd.DataFrame({\n",
    "        'document': [item['document'] for item in dataset],\n",
    "        'doc_bio_tags': [item['doc_bio_tags'] for item in dataset]\n",
    "    })\n",
    "\n",
    "# Convert datasets to dataframes\n",
    "duc2001_df = dataset_to_dataframe(duc2001_dataset)\n",
    "inspec_df = dataset_to_dataframe(inspec_dataset)\n",
    "nus_df = dataset_to_dataframe(nus_dataset)\n",
    "\n",
    "# Concatenate dataframes\n",
    "combined_df = pd.concat([duc2001_df, inspec_df, nus_df], ignore_index=True)\n",
    "\n",
    "# Convert the combined DataFrame back to a Hugging Face dataset\n",
    "combined_dataset = Dataset.from_pandas(combined_df)\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Constants\n",
    "MAX_LEN = 75\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Prepare mapping for labels\n",
    "tag2idx = {'B': 0, 'I': 1, 'O': 2}\n",
    "\n",
    "# Adjust these weights based on your specific dataset and class imbalance\n",
    "class_weights = torch.tensor([10.0, 15.0, 0.1])  # Example weights for 'B', 'I', 'O'\n",
    "# class_weights = torch.tensor([10.0, 15.0, 0.1]).cuda()  # Example weights for 'B', 'I', 'O' if GPU applicable\n",
    "\n",
    "# Tokenization and encoding for BERT\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "\n",
    "for i, item in enumerate(combined_dataset):\n",
    "    # Join tokens into a single string\n",
    "    text = ' '.join([t.lower() for t in item['document']])\n",
    "    tags = item['doc_bio_tags']\n",
    "\n",
    "    # Encode text\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Prepare labels\n",
    "    tag_ids = [tag2idx[tag] for tag in tags] + [tag2idx['O']] * (MAX_LEN - len(tags))\n",
    "    tag_ids = tag_ids[:MAX_LEN]  # Ensure label length matches input length\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'][0])\n",
    "    attention_masks.append(encoded_dict['attention_mask'][0])\n",
    "    labels.append(torch.tensor(tag_ids))\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = torch.stack(input_ids)\n",
    "attention_masks = torch.stack(attention_masks)\n",
    "labels = torch.stack(labels)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels, train_masks, val_masks = train_test_split(\n",
    "    input_ids, labels, attention_masks, test_size=0.1, random_state=2018\n",
    ")\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load BERT for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Set up the optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, eps=1e-8)  # increased learning rate\n",
    "\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 4)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# We have a class imbalance which is hindering our model performance\n",
    "# Apply focal loss to focus more on hard-to-classify examples by down-weighting the loss contributed by well-classified examples(easy-classify)\n",
    "def hybrid_loss(logits, labels, weights, alpha=0.8, gamma=2.0):\n",
    "    # Softmax and cross entropy loss\n",
    "    ce_loss = torch.nn.functional.cross_entropy(logits, labels, reduction='none', weight=weights)\n",
    "    \n",
    "    # Calculate probabilities of the true class\n",
    "    p_t = torch.exp(-ce_loss)\n",
    "    \n",
    "    # Calculate focal component\n",
    "    focal_loss = (alpha * (1 - p_t) ** gamma * ce_loss).mean()\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(4), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = hybrid_loss(outputs.logits.view(-1, 3), b_labels.view(-1), class_weights)\n",
    "\n",
    "        # # Apply class weights\n",
    "        # log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)\n",
    "        # weighted_loss = torch.nn.functional.nll_loss(log_probs.view(-1, model.num_labels), b_labels.view(-1), weight=class_weights)\n",
    "\n",
    "        # weighted_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # total_loss += weighted_loss.item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Average Training Loss: {total_loss / len(train_dataloader):.2f}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy, nb_eval_steps = 0, 0, 0\n",
    "    \n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    print(f'Validation Accuracy: {eval_accuracy / nb_eval_steps:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def update_metrics(preds, labels, metrics):\n",
    "    preds_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    tp = np.sum((preds_flat == labels_flat) & (labels_flat != tag2idx['O']))\n",
    "    fp = np.sum((preds_flat != labels_flat) & (preds_flat != tag2idx['O']))\n",
    "    fn = np.sum((preds_flat != labels_flat) & (labels_flat != tag2idx['O']))\n",
    "\n",
    "    metrics['tp'] += tp\n",
    "    metrics['fp'] += fp\n",
    "    metrics['fn'] += fn\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def calculate_scores(metrics):\n",
    "    precision = metrics['tp'] / (metrics['tp'] + metrics['fp']) if metrics['tp'] + metrics['fp'] > 0 else 0\n",
    "    recall = metrics['tp'] / (metrics['tp'] + metrics['fn']) if metrics['tp'] + metrics['fn'] > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Modified validation loop\n",
    "def validate_model(valid_dataloader, model):\n",
    "    model.eval()\n",
    "    eval_metrics = {'tp': 0, 'fp': 0, 'fn': 0}\n",
    "\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        eval_metrics = update_metrics(logits, label_ids, eval_metrics)\n",
    "\n",
    "    precision, recall, f1 = calculate_scores(eval_metrics)\n",
    "    return precision, recall, f1\n",
    "\n",
    "precision, recall, f1 = validate_model(valid_dataloader, model)\n",
    "print(f\"Validation Precision: {precision:.2f}\")\n",
    "print(f\"Validation Recall: {recall:.2f}\")\n",
    "print(f\"Validation F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and the tokenizer\n",
    "model.save_pretrained('./model_save_v5/')\n",
    "tokenizer.save_pretrained('./model_save_v5/')\n",
    "\n",
    "# Load the model and the tokenizer\n",
    "model = BertForTokenClassification.from_pretrained('./model_save_v5/')\n",
    "tokenizer = BertTokenizer.from_pretrained('./model_save_v5/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordextract(text, model, tokenizer, device):\n",
    "    # Tokenize input\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,                      # Document to encode.\n",
    "        add_special_tokens=True,   # Add '[CLS]' and '[SEP]'\n",
    "        max_length=64,             # Pad or truncate.\n",
    "        padding='max_length',      # Pad to max_length.\n",
    "        truncation=True,           # Truncate to max_length.\n",
    "        return_attention_mask=True,# Construct attention masks.\n",
    "        return_tensors='pt',       # Return PyTorch tensors.\n",
    "    )\n",
    "    \n",
    "    # Move tensors to the correct device\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Decode predictions\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    predictions = predictions[0].tolist()  # Remove the batch dimension and convert to list\n",
    "\n",
    "    # Convert input_ids to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # print(\"Tokens and Predictions:\")  # Debugging output\n",
    "    # for token, prediction in zip(tokens, predictions):\n",
    "    #     print(f\"{token}: {prediction}\")\n",
    "\n",
    "    # Extract keywords based on the 'B' and 'I' predictions\n",
    "    keywords = []\n",
    "    current_keyword = []\n",
    "    for token, pred in zip(tokens, predictions):\n",
    "        if pred == 1:  # Corresponds to 'B'\n",
    "            if current_keyword:  # Save the previous keyword if it exists\n",
    "                keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "            current_keyword = [token]  # Start a new keyword\n",
    "        elif pred == 2 and current_keyword:  # Corresponds to 'I'\n",
    "            current_keyword.append(token)\n",
    "        else:\n",
    "            if current_keyword:\n",
    "                keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "                current_keyword = []\n",
    "    \n",
    "    # Check if the last token was part of a keyword\n",
    "    if current_keyword:\n",
    "        keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Machine learning (ML) is a field of study in artificial intelligence \n",
    "concerned with the development and study of statistical algorithms that \n",
    "can learn from data and generalize to unseen data, and thus \n",
    "perform tasks without explicit instructions.\"\"\"\n",
    "keywords = keywordextract(text, model, tokenizer, device)\n",
    "print(\"Extracted Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dcg_at_k(relevance_scores, k, method=1):\n",
    "    \"\"\"Calculate discounted cumulative gain (DCG) at rank k.\n",
    "\n",
    "    Args:\n",
    "        relevance_scores (list of float): The list of relevance scores.\n",
    "        k (int): The number of results to consider.\n",
    "        method (int): The method to compute DCG, 0 or 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The DCG score.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the method is not 0 or 1.\n",
    "    \"\"\"\n",
    "    relevance_scores = np.asfarray(relevance_scores)[:k]\n",
    "    if relevance_scores.size:\n",
    "        if method == 0:\n",
    "            return relevance_scores[0] + np.sum(relevance_scores[1:] / np.log2(np.arange(2, relevance_scores.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(relevance_scores / np.log2(np.arange(2, relevance_scores.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(relevance_scores, k, method=1):\n",
    "    \"\"\"Calculate normalized discounted cumulative gain (NDCG) at rank k.\n",
    "\n",
    "    Args:\n",
    "        relevance_scores (list of float): The list of relevance scores.\n",
    "        k (int): The number of results to consider.\n",
    "        method (int): The method to compute DCG, 0 or 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The NDCG score.\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(relevance_scores, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.0\n",
    "    return dcg_at_k(relevance_scores, k, method) / dcg_max\n",
    "\n",
    "def mean_reciprocal_rank(ranking_lists):\n",
    "    \"\"\"Calculate the mean reciprocal rank (MRR).\n",
    "\n",
    "    Args:\n",
    "        ranking_lists (list of list of int): Each inner list is a set of binary values (0 or 1)\n",
    "            indicating the absence or presence of relevant items.\n",
    "\n",
    "    Returns:\n",
    "        float: The MRR score.\n",
    "    \"\"\"\n",
    "    first_relevant = (np.asarray(rankings).nonzero()[0] for rankings in ranking_lists)\n",
    "    return np.mean([1.0 / (ranking[0] + 1) if ranking.size else 0 for ranking in first_relevant])\n",
    "\n",
    "def calculate_relevance_scores(true_keywords, predicted_keywords):\n",
    "    \"\"\"Calculates relevance scores where 1 indicates relevance and 0 indicates irrelevance.\n",
    "   \n",
    "    Args:\n",
    "        true_keywords (list of str): The list of true keywords.\n",
    "        predicted_keywords (list of tuples): List of predicted keywords with their scores.\n",
    "   \n",
    "    Returns:\n",
    "        list of int: Relevance scores (1 or 0) for each predicted keyword.\n",
    "    \"\"\"\n",
    "    return [1 if keyword in true_keywords else 0 for keyword, _ in predicted_keywords]\n",
    "\n",
    "def evaluate_keyword_extraction(true_data, predictions):\n",
    "    \"\"\"Evaluates the keyword extraction algorithm using NDCG and MRR scoring metrics.\n",
    "   \n",
    "    Args:\n",
    "        true_data (list of list of str): List of lists containing true keywords for each document.\n",
    "        predictions (list of list of tuples): List of lists, each containing tuples of keywords and their confidence scores.\n",
    "   \n",
    "    Returns:\n",
    "        tuple of (float, float): Mean NDCG score and Mean MRR score.\n",
    "    \"\"\"\n",
    "    ndcg_scores = []\n",
    "    mrr_scores = []\n",
    "\n",
    "    for true_keywords, predicted_keywords_with_scores in zip(true_data, predictions):\n",
    "        predicted_keywords_with_scores.sort(key=lambda x: x[1], reverse=True)  # Sort by confidence score descending\n",
    "        predicted_keywords = [kw for kw, _ in predicted_keywords_with_scores]\n",
    "        relevance_scores = calculate_relevance_scores(true_keywords, predicted_keywords_with_scores)\n",
    "\n",
    "        print(\"Predicted Keywords with Scores After Sorting:\", predicted_keywords_with_scores)\n",
    "\n",
    "\n",
    "        print(\"True Keywords:\", true_keywords)\n",
    "        print(\"Predicted Keywords:\", predicted_keywords)\n",
    "        print(\"Relevance Scores:\", relevance_scores)\n",
    "\n",
    "        # Compute NDCG\n",
    "        ndcg_score = ndcg_at_k(relevance_scores, k=len(relevance_scores))\n",
    "        ndcg_scores.append(ndcg_score)\n",
    "       \n",
    "        # Compute MRR\n",
    "        rs = [[1 if keyword in true_keywords else 0 for keyword in predicted_keywords]]\n",
    "        mrr_score = mean_reciprocal_rank(rs)\n",
    "        mrr_scores.append(mrr_score)\n",
    "   \n",
    "    mean_ndcg = np.mean(ndcg_scores)\n",
    "    mean_mrr = np.mean(mrr_scores)\n",
    "    return mean_ndcg, mean_mrr\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
